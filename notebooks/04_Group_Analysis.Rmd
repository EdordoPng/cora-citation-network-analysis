---
title: "Group Analysis"
author: "Edoardo Diana"
date: "2025-11-20"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

--------------------------------------------------------------------------------

## Indice

  1 : Carico librerie e grafo
  
  2 : Filtro sui nodi 
  
  3 : Community detection e Clustring

  4 : Modularità

  5 : Ottimizzazzione della modularity (Algoritmi di Community Detection) :


      5.1 Louvain     
  
        Grafo : Network con raggruppamento ottenuto
        Grafo : Network con raggruppamento Ground Truth (Categorie CoraNetwork)
  
      5.2 Label Propagation
  
      5.3 Infomap community finding

      5.4 Clustering Gerarchico                           

        Grafo : Struttura principale Dendrogramma

  6 Domande chiave:

      A. Le communities coincidono con le categorie scientifiche?

      B. Quali categorie sono più pure ? (una sola community o meno possibili)

  7 : Salvatoaggio Risultati ottenuti

--------------------------------------------------------------------------------

## 1 Carico le librerie ed il grafo

H oottenuto il grafo dal notebook precedente.

```{r}
library(igraph)
library(ggplot2)
library(ggraph)
library(dplyr)
library(tidygraph)
library(knitr)
```

Carico il grafo con Rao ottenuto dal notebook 03

```{r}
load("../data/processed/graph_with_rao.RData")

cat("Grafo caricato:\n")
cat(" Nodi:", vcount(graph), "\n")
cat(" Archi:", ecount(graph), "\n")
cat(" Categorie ground-truth:", length(unique(V(graph)$category)), "\n")

```


## 2 : Filtro i nodi

Lavoriamo su un insieme di nodi minore dell'originale, il quale è troppo sparso.

Settp una soglia minima di 5 come grado totale (in degree + out degree del nodo).

```{r}

# Calcolo del degree totale (in + out)
total_degree <- V(graph)$indegree + V(graph)$outdegree

# Soglia minima
min_degree <- 5

# Identifichiamo i nodi da mantenere
keep_nodes <- which(total_degree >= min_degree)

cat("Filtro applicato:\n")
cat("  Degree minimo (in+out): >=", min_degree, "\n")
cat("  Nodi mantenuti:", length(keep_nodes), 
    paste0("(", round(length(keep_nodes) / vcount(graph) * 100, 1), "%)"), "\n")
cat("  Nodi rimossi:", vcount(graph) - length(keep_nodes), 
    paste0("(", round((vcount(graph) - length(keep_nodes)) / vcount(graph) * 100, 1), "%)"), "\n\n")

# Creo il sottografo filtrato
graph_filtered <- induced_subgraph(graph, keep_nodes)

cat("Sottografo filtrato:\n")
cat("  Nodi:", vcount(graph_filtered), "\n")
cat("  Archi:", ecount(graph_filtered), "\n")
cat("  Densità:", round(ecount(graph_filtered) / (vcount(graph_filtered) * (vcount(graph_filtered) - 1) / 2), 4), "\n\n")

```


## 3 : Community Detection e Clustering

Il Community Detection è il problema di trovare una divisione naturale della rete in comunintà.

Le community sono gruppi di nodi densamente connessi.

A priori non sappiammo ne i gruppi ne il numero dei gruppi, ne veniamo a capo con un algoritmo.


## 4 : Modularità 

La Modularity Q misura la qualità della partizione (quanto bene una rete è divisa in communità).

E' un numero che quantifica la proprietà di avete tanti edge intra-gruppo e poche inter-gruppo.
Noi vogliamo cercare di massimizzarla.

Questa è la Modularity matrix Q :

$$
Q = \frac{1}{2m} \sum_{ij} \left(A_{ij} - \frac{k_i k_j}{2m}\right) \delta(c_i, c_j)
$$

Dove:

  * A_{ij} = matrice di adiacenza
  * k_i, k_j = gradi dei nodi
  * m = numero di archi
  * delta(c_i, c_j) = 1 se i e j nella stessa community, 0 altrimenti.

Ogni componente della matrice di modularità ha quella differenza tra i rispettivi nodi i e j scelti.

Poichè cercare di massimizzare la modularità è un Computationally hard probelm, serve ricorrere ad Heuristic Algorithms.
Cercano di massimizzare la modularità in modi intelligenti.


## 5 : Community Detection Algorithms sulla rete che ottimizzano la Modularità

Abbiamo dunque filtratoi il grafo, il quale è ancora un grafo diretto non pesato.

Serve ora far collassare gli archi bidirezionali, ossia convertiamo il grafo.

PAssiamo da uno diretto a non diretto.

```{r}

# Converto in non-diretto
graph_filtered <- as.undirected(graph_filtered, mode = "collapse")


cat("Grafo convertito:\n")
cat("  Nodi:", vcount(graph_filtered), "\n")
cat("  Archi:", ecount(graph_filtered), "\n")
cat("  Densità:", round(ecount(graph_filtered) / (vcount(graph_filtered) * (vcount(graph_filtered) - 1) / 2), 4), "\n")
cat("  Diretto:", is_directed(graph_filtered), "\n\n")

```
Nota che ora ho 77 archi in meno, questo perchè in aluni casi avevo una coppia di nodi che si citavano a vicenda.


Questi sotto sono tutti gli Heuristic Algorithm che analizzeremo.

```{r}
# Lista degli algoritmi di community detection
methods <- list(
  "Edge Betweenness" = cluster_edge_betweenness,
  "Fast Greedy" = cluster_fast_greedy,
  "Label Propagation" = cluster_label_prop,
  "Leading Eigenvector" = cluster_leading_eigen,
  "Louvain" = cluster_louvain,
  "Walktrap" = cluster_walktrap,
  "Infomap" = cluster_infomap
)
```

Creo ora iun dataframe per contenere i risultati dell'applicazione di essi alla rete.

```{r}
# Dataframe per i risultati
results <- data.frame(
  Method = character(), 
  Communities = integer(),
  Modularity = numeric(), 
  Time_seconds = numeric(),
  stringsAsFactors = FALSE
)
```

Questo ciclo scorre ogni elemento della lista methods, procede poi con l'applicazione di esso ed al salvataggio dei risultati.

```{r}
# Eseguiamo ogni algoritmo
for (method in names(methods)) {
  cat("  -", method, "... ")
  
  tryCatch({
    # Misura tempo di esecuzione
    start_time <- Sys.time()
    
    # Detect communities
    set.seed(42)  # Per riproducibilità
    communities <- methods[[method]](graph_filtered)
    
    end_time <- Sys.time()
    time_taken <- as.numeric(difftime(end_time, start_time, units = "secs"))
    
    # Calcola modularity
    modularity_value <- modularity(communities)
    n_communities <- length(communities)
    
    # Salva risultati
    results <- rbind(results, data.frame(
      Method = method, 
      Communities = n_communities,
      Modularity = modularity_value,
      Time_seconds = time_taken
    ))
    
    cat("OK (Q =", round(modularity_value, 4), ")\n")
    
  }, error = function(e) {
    cat("ERRORE:", e$message, "\n")
  })
}

```


Ordiniamo i risultati ottenuti per modularity decrescente e mostro i dati con una tabella.


```{r}
results <- results[order(-results$Modularity), ]

kable(results, 
      digits = c(0, 0, 4, 3),
      col.names = c("Algoritmo", "Communities", "Modularity Q", "Tempo (sec)"),
      caption = "Confronto algoritmi di community detection")


# Statistiche
cat("Miglior algoritmo (Modularity):", results$Method[1], 
    paste0("(Q = ", round(results$Modularity[1], 4), ")"), "\n")
cat("Algoritmo più veloce:", results$Method[which.min(results$Time_seconds)],
    paste0("(", round(min(results$Time_seconds), 3), " sec)"), "\n")
cat("Algoritmo più lento:", results$Method[which.max(results$Time_seconds)],
    paste0("(", round(max(results$Time_seconds), 3), " sec)"), "\n\n")

```


### 5.1 Louvain

Ricorda che staimo lavorando sul sottografo tale che ogni nodo abbiam un total degree di almeno 5.

Applico dunque Louvain sul sottografo.

```{r fig.width=12, fig.height=9, fig.align='center', warning=FALSE}

set.seed(42)
louvain_filtered <- cluster_louvain(graph_filtered)

cat("Risultati Louvain (sottografo):\n")
cat("  Communities trovate:", length(louvain_filtered), "\n")
cat("  Modularity Q:", round(modularity(louvain_filtered), 4), "\n\n")


# Distribuzione dimensioni communities
filtered_sizes <- sort(table(membership(louvain_filtered)), decreasing = TRUE)

cat("Distribuzione dimensioni communities:\n")
print(filtered_sizes)
cat("\n")

cat("Statistiche:\n")
cat("  Dimensione media:", round(mean(filtered_sizes), 1), "nodi\n")
cat("  Dimensione mediana:", median(filtered_sizes), "nodi\n")
cat("  Community più grande:", max(filtered_sizes), "nodi\n")
cat("  Community più piccola:", min(filtered_sizes), "nodi\n\n")

# Aggiungo membership al sottografo
V(graph_filtered)$community <- membership(louvain_filtered)


```

Mostro ora 2 grafi, correispondenti al raggruppamento dei nodi fatto da Louvain e quello di confronto, ossia le categoria del paper nella Cora Network.

```{r fig.width=12, fig.height=9, fig.align='center', warning=FALSE}

# Converto il sottografo in tidygraph
graph_tidy <- as_tbl_graph(graph_filtered) %>%
  mutate(community = as.factor(community))


# Layout
set.seed(42)

p <- ggraph(graph_tidy, layout = 'fr') +
  # Archi
  geom_edge_link(aes(), 
                 color = "gray80", 
                 width = 0.2, 
                 alpha = 0.3) +
  # Nodi
  geom_node_point(aes(color = community), 
                  size = 2.5, 
                  alpha = 0.8) +
  # Tema e stile
  scale_color_viridis_d(option = "turbo", name = "Community") +
  labs(title = "Louvain Community Detection (grafo filtrato)",
       subtitle = paste0(vcount(graph_filtered), " nodi (degree ≥", min_degree, ") | ",
                        length(louvain_filtered), " communities, Q = ", 
                        round(modularity(louvain_filtered), 3))) +
  theme_graph(base_family = "sans") +
  theme(legend.position = "bottom",
        legend.direction = "horizontal",
        legend.title = element_text(size = 9, face = "bold"),
        legend.text = element_text(size = 7),
        legend.key.size = unit(0.4, "cm"),     
        legend.box.spacing = unit(0.1, "cm"), 
        plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
        plot.subtitle = element_text(size = 11, hjust = 0.5)) +
  guides(color = guide_legend(nrow = 2, byrow = TRUE)) 

print(p)



# Visualizzazione Ground-Truth (categorie)

# Converto il sottografo in tidygraph con categorie
graph_tidy_cat <- as_tbl_graph(graph_filtered) %>%
  mutate(category = as.factor(category))


# Stesso layout del primo grafico per confronto diretto
set.seed(42)

# Creo il plot con ggraph settando il colori in base alle categorie
p_cat <- ggraph(graph_tidy_cat, layout = 'fr') +
  # Archi
  geom_edge_link(aes(), 
                 color = "gray80", 
                 width = 0.2, 
                 alpha = 0.3) +
  # Nodi (colore = categoria)
  geom_node_point(aes(color = category), 
                  size = 2.5, 
                  alpha = 0.8) +
  # Tema e stile
  scale_color_brewer(palette = "Set2", name = "Categoria") +
  labs(title = "Ground-Truth Categories (grafo filtrato)",
       subtitle = paste0(vcount(graph_filtered), " nodi (degree ≥", min_degree, ") | ",
                        length(unique(V(graph_filtered)$category)), " categorie Cora")) +
  theme_graph(base_family = "sans") +
  theme(legend.position = "bottom",
        legend.direction = "horizontal",
        legend.title = element_text(size = 10, face = "bold"),
        legend.text = element_text(size = 9),
        plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
        plot.subtitle = element_text(size = 11, hjust = 0.5))

print(p_cat)

```



### 5.2 : Label Propagation

Ad ogni nodo viene assegnata 1 di k etichette.

Il metodo poi procede iterativamente a reassegnare le etichette.

Si ferma quando l'etichetta di ogni nodo è una delle etichette più freuqneti nel suo vicinato.

Putroppo il problema di questo algoritmo è che su grafi diretti può non convergere e andare in loop infinito.


### 5.3 : Cluster Infomap

Tenta di creare un raggruppamento che fornisca la "shortest description length" per una random walk sul grafo.

La lunghezza della descrizione è misurata mediante il numero atteso di bit per vertice che son richiesti per codificare il percorso di una random walk.

Un random walker che "naviga" seguendo le citazioni tenderà a rimanere all'interno di gruppi di paper tematicamente correlati.

Putroppo facevo varie prove questo algorimo identifica comunque troppe categorie, dunque inapplicabile per questo grafo diretto.


### 5.4 Clustering Gerarchico

L'idea è che se ho dei punti in uno spazio, posso far dei cluster considerando la distanza euclidea.

Definisce una misura della forza della connessione tra nodi, basata sulla struttura della rete.

Ogni nodo parte in un proprio gruppo, poi si joina in un gruppo solo quelle coppie di nodi con la similarità più alta.
Poi farò delle join con i gruppi che son più simili.

Nota che dopo aver unito 2 gruppi, serve ricalcolare le metriche in merito alla distanza.


Prendo la matrice di adiacenza del grado e valuto la Similarità coseno tra i nodi.

```{r}

# Matrice di adiacenza
A <- as_adjacency_matrix(graph_filtered, sparse = FALSE)

cat("Matrice di adiacenza:\n")
cat("  Dimensione:", nrow(A), "×", ncol(A), "\n")
cat("  Densità:", round(sum(A) / (nrow(A) * ncol(A)), 4), "\n\n")



# Similarità Coseno (con gestione nodi isolati)

# Funzione distanza euclidea per normalizzazione
euclidean <- function(x) sqrt(sum(x * x))

# Calcolo la norma di ogni colonna (nodo)
d <- apply(A, 2, euclidean)

# Identifica nodi con norma zero (isolati)
zero_degree <- which(d == 0)

if (length(zero_degree) > 0) {
  cat("Trovati", length(zero_degree), "nodi con degree 0.\n")
  cat("Questi nodi verranno rimossi prima del clustering.\n\n")
  
  # Rimuovo i nodi isolati
  A <- A[-zero_degree, -zero_degree]
  d <- d[-zero_degree]
  
  cat("Matrice ridotta:\n")
  cat("  Dimensione:", nrow(A), "×", ncol(A), "\n\n")
}

# Matrice diagonale delle norme inverse (evita divisione per zero)
D <- diag(1 / d)

# Similarità coseno: S = D * A^T * A * D
S_cosine <- D %*% t(A) %*% A %*% D

# Diagonale a 1 (similarità di un nodo con se stesso)
diag(S_cosine) <- 1

# Conversione della similarità in distanza
D_cosine <- 1 - S_cosine

# Nel caso di eventuali valori negativi da errori numerici
D_cosine[D_cosine < 0] <- 0

# Accerto la simmetria
D_cosine <- (D_cosine + t(D_cosine)) / 2


# Converto in oggetto dist
dist_cosine <- as.dist(D_cosine)

# Gestisco la presenza di NA/NaN/Inf
if (any(is.na(dist_cosine)) || any(is.infinite(dist_cosine))) {
  cat("ERRORE: Valori NA/NaN/Inf nella matrice distanza!\n")
  cat("Sostituisco con valori validi...\n\n")
  
  # Sostituisco NA con distanza massima
  dist_matrix <- as.matrix(dist_cosine)
  dist_matrix[is.na(dist_matrix)] <- max(dist_matrix[!is.na(dist_matrix)])
  dist_matrix[is.infinite(dist_matrix)] <- max(dist_matrix[!is.infinite(dist_matrix)])
  dist_cosine <- as.dist(dist_matrix)
}

cat("Similarità coseno calcolata.\n")
cat("Range distanze:", round(range(dist_cosine), 3), "\n")
cat("Valori NA:", sum(is.na(dist_cosine)), "\n")
cat("Valori Inf:", sum(is.infinite(dist_cosine)), "\n\n")

```

Eseguiamo lo hierarchical clustering.

```{r fig.width=12, fig.height=9, fig.align='center', warning=FALSE}

# Clustering gerarchico
hc_cosine <- hclust(dist_cosine, method = "average")

# Converto in dendrogramma
dend <- as.dendrogram(hc_cosine)

# Tagliamo ad un'altezza intermedia per mostrare solo la struttura alta
cut_height_display <- 0.8

# Tagliamo il dendrogramma
dend_cut <- cut(dend, h = cut_height_display)

# Plot del dendrogramma alto
par(mar = c(4, 4, 3, 2))
plot(dend_cut$upper, 
     main = paste0("Dendrogramma - Struttura principale\n(altezza > ", cut_height_display, ")"),
     xlab = "",
     ylab = "Distanza (1 - Cosine Similarity)",
     nodePar = list(pch = 19, cex = 0.8, col = "steelblue"))

```

Estrazione clusters.

Ho fatto vari test cambiando k_clusters, alla fine il valore che massimizza la modularity è 31 cluster.

```{r}

k_clusters <- 31
# Linea per k clusters
cut_height_k <- hc_cosine$height[length(hc_cosine$height) - k_clusters + 1]


# Tagliamo il dendrogramma a k clusters
clusters_hc <- cutree(hc_cosine, k = k_clusters)

# Mapiamo i clusters ai nodi originali
if (length(zero_degree) > 0) {
  # Crea vettore completo
  clusters_full <- rep(NA, vcount(graph_filtered))
  clusters_full[-zero_degree] <- clusters_hc
  
  # Assegno i nodi isolati a cluster separato
  clusters_full[zero_degree] <- k_clusters + 1
  
  V(graph_filtered)$cluster_hc <- clusters_full
  
  cat("Nodi isolati assegnati a cluster", k_clusters + 1, "\n\n")
} else {
  V(graph_filtered)$cluster_hc <- clusters_hc
}

```

Mostriamo qui le varie statistiche 

```{r}
cluster_sizes <- sort(table(V(graph_filtered)$cluster_hc), decreasing = TRUE)

cat("Clusters estratti (k =", k_clusters, "):\n\n")
cat("Distribuzione dimensioni:\n")
print(cluster_sizes)
cat("\n")

cat("Statistiche:\n")
cat("  Dimensione media:", round(mean(cluster_sizes), 1), "nodi\n")
cat("  Dimensione mediana:", median(cluster_sizes), "nodi\n")
cat("  Cluster più grande:", max(cluster_sizes), "nodi\n")
cat("  Cluster più piccolo:", min(cluster_sizes), "nodi\n\n")

# Calcola modularity
hc_communities <- make_clusters(graph_filtered, membership = V(graph_filtered)$cluster_hc)
hc_modularity <- modularity(hc_communities)

cat("Modularity Q:", round(hc_modularity, 4), "\n\n")

```

Il valore ottenuto della modularity non è male,  Q: 0.7194  

Ma è comunqu più basso rispetto quello che avevamo ottenuto con gli altri Heuristic algorithms.



## 6 : Domande


## Domanda A : Le communities coincidono con le categorie scientifiche?

Abbiamo 7 Categorie vs 34 Communities trovate mediante Louvain

Cerchiamo di capire quale cattura meglio la struttura della rete.

L'idea è di eseguire il calcolo della modularity per diversi livelli di granularità.

La granularità indica quanto “fine” o “grossolana” sia la partizione della rete svolta dagli algoritmi di clustering o community detection.

Bassa granularità: pochi gruppi molto grandi.

Alta granularità: tanti gruppi piccoli e specifici.

```{r fig.width=12, fig.height=9, fig.align='center', warning=FALSE}

# Estraggo i dati dal grafo
communities_best <- V(graph_filtered)$community  # Membership Louvain
categories <- V(graph_filtered)$category         # Categorie ground-truth


# Calcolo della modularity per diversi livelli di granularità
granularity_results <- data.frame(
  K = integer(),
  Source = character(),
  Modularity = numeric(),
  Avg_size = numeric(),
  Min_size = integer(),
  Max_size = integer(),
  stringsAsFactors = FALSE
)
```

Ho qui 3 casi : 

  1. Ground-truth (7 categorie)
  
  2. Louvain (communities già calcolate)
  
  3. Louvain forzato a k≈7


Nota che l'ultimo caso è particolare, dato che in realtà Louvain non permette di settare un numero di communities desiderate.

target_k <- 7 singifica ch  cerco di ottenere circa 7 cluster.

resolution_values: è un vettore di valori che modifica la “granularità” dell’algoritmo Louvain. 

Il ciclo tenta diverse soglie di resolution tra 0.2 e 2.0.

Per ogni valore, Louvain viene applicato al grafo filtrato con quella risoluzione.

Si misura k_test, il numero di cluster trovati con quella configurazione.


Si tiene traccia del numero di cluster “migliore”, cioè quello che si avvicina di più a target_k.

Alla fine del ciclo, scegli il risultato (in termini di partizione) che produce il numero di cluster più vicino al target.


```{r}

# 1. Ground-truth (7 categorie)
cat_membership <- as.numeric(factor(categories))
cat_communities <- make_clusters(graph_filtered, membership = cat_membership)
cat_modularity <- modularity(cat_communities)
cat_sizes <- table(cat_membership)

granularity_results <- rbind(granularity_results, data.frame(
  K = length(unique(cat_membership)),
  Source = "Ground-Truth (Categorie)",
  Modularity = round(cat_modularity, 4),
  Avg_size = round(mean(cat_sizes), 1),
  Min_size = min(cat_sizes),
  Max_size = max(cat_sizes)
))

# 2. Louvain (communities già calcolate)
louv_communities <- make_clusters(graph_filtered, membership = communities_best)
louv_modularity <- modularity(louv_communities)
louv_sizes <- table(communities_best)

granularity_results <- rbind(granularity_results, data.frame(
  K = length(unique(communities_best)),
  Source = "Louvain (ottimale)",
  Modularity = round(louv_modularity, 4),
  Avg_size = round(mean(louv_sizes), 1),
  Min_size = min(louv_sizes),
  Max_size = max(louv_sizes)
))

# 3. Louvain forzato a k≈7
target_k <- 7
resolution_values <- seq(0.2, 2.0, by = 0.2)
best_diff <- Inf
best_louv_k7 <- NULL

for (res in resolution_values) {
  set.seed(42)
  test_louv <- cluster_louvain(graph_filtered, resolution = res)
  k_test <- length(test_louv)
  
  if (abs(k_test - target_k) < best_diff) {
    best_diff <- abs(k_test - target_k)
    best_louv_k7 <- test_louv
  }
}

louv_k7_membership <- membership(best_louv_k7)
louv_k7_modularity <- modularity(best_louv_k7)
louv_k7_sizes <- table(louv_k7_membership)

granularity_results <- rbind(granularity_results, data.frame(
  K = length(best_louv_k7),
  Source = "Louvain (k≈7 forzato)",
  Modularity = round(louv_k7_modularity, 4),
  Avg_size = round(mean(louv_k7_sizes), 1),
  Min_size = min(louv_k7_sizes),
  Max_size = max(louv_k7_sizes)
))

```

Facciamo infine un confronto sulla Granularità.

Creo qui una struttura automatizzata che produce l'output testuale che spieghi il caso in base ai risultati riscontrati.

```{r}
print(granularity_results, row.names = FALSE)


mod_categories <- granularity_results$Modularity[1]
mod_louvain <- granularity_results$Modularity[2]
mod_louvain_k7 <- granularity_results$Modularity[3]

```

1. Le 7 categorie della Cora network risultano essr troppo grossolane.

2. Louvain (k= 34) trova una struttura più modulare che cattura meglio le sotto comunità.

   Modularity migliorata del 23.7 %
  
3. Louvain forzato a k≈7:
  
   Ottiene k = 25 con Modularità Q = 0.8888 
    


Mostro qui un plot comparativo

```{r}
p_granularity <- ggplot(granularity_results, aes(x = K, y = Modularity, color = Source)) +
  geom_point(size = 5, alpha = 0.8) +
  geom_line(aes(group = 1), linetype = "dashed", alpha = 0.5, color = "gray50") +
  geom_text(aes(label = paste0("Q=", Modularity)), 
            vjust = -1.2, size = 3.5, show.legend = FALSE) +
  scale_color_brewer(palette = "Set1", name = NULL) +
  labs(title = "Confronto Granularità: Categorie vs Communities",
       subtitle = paste0(vcount(graph_filtered), " nodi"),
       x = "Numero di cluster (k)", 
       y = "Modularity Q") +
  theme_minimal() +
  theme(legend.position = "bottom",
        plot.title = element_text(face = "bold", size = 14, hjust = 0.5),
        plot.subtitle = element_text(size = 11, hjust = 0.5))

print(p_granularity)
```


I risultati rivelano un disallineamento tra la categorizzazione scientifica nella Cora Nwteork e la struttura della rete (solo nodi con degree > 5):

1. **Ground-truth (k=7, Q=0.641)**: 

  - Le categorie scientifiche hanno una modularity moderata, la classificazione non cattura perfettamente i pattern di citazione.

2. **Louvain ottimale (k=34, Q=0.793)**: 

  - L'algoritmo converge a 34 communities, migliorando la modularity del 23.7%. 
  - Questo indica l'esistenza di sottocategorie tematiche** (es. CNN o RNN all'interno di Neural Networks).

3. **Louvain calibrato (k=25, Q=0.889)**: 

   Cercando di forzare k≈7, l'algoritmo converge a k=25 con modularity massima (+38.8%). 
   
   Questo suggerisce che:
   
      - La granularità ottimale della rete è ~25 sub-discipline
      - Le 7 categorie sono macro-raggruppamenti grossolani
      - La struttura di citazione riflette specializzazioni più fini


## Domanda B : Quali categorie sono "pure" (una sola community)?

Costruiamo una tabella di frammentazione e purezza per ciascuna categoria (ground-truth) nel grafo.

Lo facciamo incrociando la partizione fornita dall’algoritmo di community detection (Louvain) con la classificazione originale delle categorie. 

Così da poter capire quanto le categorie sono “coese” o “frammentate” rispetto alle community trovate.

```{r}

# Estrae la community Louvain assegnata a ogni nodo
community_per_nodo <- V(graph_filtered)$community   

# Estrae la categoria ground-truth di ogni nodo
categoria_per_nodo <- V(graph_filtered)$category    

# Prepara l'elenco unico delle categorie
categories <- unique(categoria_per_nodo)    

# Inizializzo un dataframe vuoto che conterrà: nome categoria, entropia, numero di sub-community, purezza massima
fragplotdata <- data.frame(
  Category = character(),
  Entropy = numeric(),
  N_communities = integer(),
  Largest_pct = numeric(),
  stringsAsFactors = FALSE
)


for(cat in categories) {
  
  # Trova gli indici (id dei nodi) di tutti quelli che appartengono alla categoria cat.
  nodes_in_cat <- which(categoria_per_nodo == cat)  
  
  # Recupero, per questi nodi, l'indicazione della community Louvain a cui sono stati assegnati.
  communities_in_cat <- community_per_nodo[nodes_in_cat]

  # Conta quanti nodi della categoria cat sono assegnati a ciascuna community Louvain.
  tab <- table(communities_in_cat)   

  # Calcola la proporzione (frequenza relativa) di nodi della categoria cat in ognuna delle sue sub-community.
  probs <- tab / sum(tab)              

  # Calcola l'entropia di Shannon delle proporzioni (distribuzione fra diverse sub-community): 
  # valore alto = dispersa, valore basso = coesa.
  entropy <- -sum(probs * log2(probs))  

  # Calcola la purezza massima: percentuale di nodi cat che stanno nella community dominante.
  largest_pct <- max(probs) * 100       

  fragplotdata <- rbind(                 
    fragplotdata,
    data.frame(
      Category = cat,
      Entropy = entropy,
      N_communities = length(tab),
      Largest_pct = largest_pct
    )
  )
}


```

Plotto qui la Frammentazione e la Purezza delle categorie della cora Network.

```{r fig.width=12, fig.height=9, fig.align='center', warning=FALSE}

frag_plot_data <- fragplotdata
frag_plot_data$Category_short <- gsub("_", "\n", frag_plot_data$Category)

# Ordino per entropia
frag_plot_data <- frag_plot_data[order(frag_plot_data$Entropy), ]
frag_plot_data$Category_short <- factor(frag_plot_data$Category_short, 
                                         levels = frag_plot_data$Category_short)


# Plot 1 

p1 <- ggplot(frag_plot_data, aes(x = Category_short)) +
  # Barre entropia
  geom_col(aes(y = Entropy, fill = "Entropia\n(frammentazione)"), alpha = 0.7) +
  # Linea numero communities (scala secondaria)
  geom_line(aes(y = N_communities / 6, group = 1, color = "N Sub-Communities"), 
            size = 1.5) +
  geom_point(aes(y = N_communities / 6, color = "N Sub-Communities"), 
             size = 4) +
  # Annotazioni
  annotate("text", x = 1.5, y = 3.5, 
           label = "Entropia alta = Categoria frammentata\n(divisa in molte sub-communities)",
           size = 3.5, color = "coral", fontface = "italic") +
  annotate("text", x = 1.5, y = 0.5, 
           label = "Entropia bassa = Categoria coesa\n(concentrata in poche communities)",
           size = 3.5, color = "darkgreen", fontface = "italic") +
  # Scale
  scale_y_continuous(
    name = "Entropia (misura frammentazione, 0-4 bits)",
    sec.axis = sec_axis(~.*6, name = "Numero Sub-Communities")
  ) +
  scale_fill_manual(values = c("Entropia\n(frammentazione)" = "coral"), name = NULL) +
  scale_color_manual(values = c("N Sub-Communities" = "steelblue"), name = NULL) +
  labs(title = "Frammentazione delle Categorie",
       subtitle = "Entropia di Shannon (barre) + Numero di sub-communities (linea)",
       x = NULL) +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold", hjust = 0.5, size = 15),
        plot.subtitle = element_text(hjust = 0.5, size = 11),
        axis.text.x = element_text(size = 10),
        legend.position = "bottom",
        legend.text = element_text(size = 11))

print(p1)


# Plot 2: Lollipop chart - Purezza per categoria

frag_plot_data <- frag_plot_data[order(-frag_plot_data$Largest_pct), ]
frag_plot_data$Category_short <- factor(frag_plot_data$Category_short, 
                                         levels = frag_plot_data$Category_short)

# Aggiungi colore per soglie
frag_plot_data$Purezza_level <- cut(frag_plot_data$Largest_pct,
                                     breaks = c(0, 50, 70, 100),
                                     labels = c("Bassa (<50%)", 
                                                "Media (50-70%)", 
                                                "Alta (>70%)"))

p2 <- ggplot(frag_plot_data, aes(x = Category_short, y = Largest_pct, 
                                  color = Purezza_level)) +
  geom_segment(aes(xend = Category_short, y = 0, yend = Largest_pct), 
               size = 1.5) +
  geom_point(size = 6) +
  geom_text(aes(label = paste0(Largest_pct, "%")), 
            vjust = -1, size = 4, color = "black", fontface = "bold") +
  geom_hline(yintercept = 50, linetype = "dashed", color = "red", size = 0.8) +
  annotate("text", x = 0.7, y = 53, label = "Soglia 50%", 
           color = "red", size = 3.5) +
  scale_color_manual(values = c("Bassa (<50%)" = "#d62728", 
                                 "Media (50-70%)" = "#ff7f0e",
                                 "Alta (>70%)" = "#2ca02c"),
                     name = "Livello Purezza") +
  coord_flip() +
  labs(title = "Purezza delle Categorie",
       subtitle = "% nodi nella community dominante",
       x = NULL, y = "Purezza (%)") +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold", hjust = 0.5, size = 15),
        plot.subtitle = element_text(hjust = 0.5, size = 11),
        legend.position = "bottom",
        axis.text = element_text(size = 10))

print(p2)

```


Entropia BASSA (es. 1.065) → Categoria coesa

  - La maggior parte dei nodi è in 1-2 communities.

  - Esempio: Reinforcement Learning (81.8% in una community)



Entropia ALTA (es. 3.384) → Categoria frammentata

  - I nodi sono sparsi in molte communities diverse.

  - Esempio: Neural Networks (divisa in 19 communities!)



Tutte le categorie sono frammentate.


Neural Networks (PIÙ frammentata):

  - Divisa in 19 sub-communities diverse

  - Solo 25.7% dei nodi nella community principale

  - Entropia 3.384 (altissima)

Non esiste un unico argomento "Neural Networks" unificato, ma si divide in CNN, RNN, GAN, Transformers, etc.


Reinforcement Learning (MENO frammentata):

  - 81.8% dei nodi in una sola community (C7)

  - Ma comunque divisa in 8 communities → ancora frammentata


La struttura reale ha ~25 sub-communities specializzate invece delle 7 categorie di partenza.



### Analizziamo ora come estensione la frammentazione e la purezza delle 24 communities trovate mediante Louvain

```{r}

louv_k7_membership <- membership(best_louv_k7)  # ad esempio 24 cluster trovati

community_ids <- unique(louv_k7_membership)
community_fragmentation <- data.frame(
  Community = integer(),
  Entropy = numeric(),
  N_categories = integer(),
  Largest_pct = numeric(),
  stringsAsFactors = FALSE
)

for (comm in community_ids) {
  nodes_in_comm <- which(louv_k7_membership == comm)
  categories_in_comm <- categoria_per_nodo[nodes_in_comm]
  tab <- table(categories_in_comm)
  probs <- tab / sum(tab)
  entropy <- -sum(probs * log2(probs))
  largest_pct <- max(probs) * 100
  community_fragmentation <- rbind(
    community_fragmentation,
    data.frame(
      Community = comm,
      Entropy = entropy,
      N_categories = length(tab),
      Largest_pct = largest_pct
    )
  )
}

```

Vediamo ora il plot come fatto in precedenza

```{r fig.width=12, fig.height=9, fig.align='center', warning=FALSE}

community_plot_data <- community_fragmentation
community_plot_data$Community_short <- paste0("Comm\n", community_plot_data$Community)

# Ordino per entropia, imposto i livelli per l'asse X
community_plot_data <- community_plot_data[order(community_plot_data$Entropy), ]
community_plot_data$Community_short <- factor(community_plot_data$Community_short, 
                                              levels = community_plot_data$Community_short)

# --- Plot entropia e numero categorie all'interno delle community ---

p_comm1 <- ggplot(community_plot_data, aes(x = Community_short)) +
  # Barre entropia
  geom_col(aes(y = Entropy, fill = "Entropia\n(frammentazione)"), alpha = 0.7) +
  # Linea numero categorie (scala secondaria)
  geom_line(aes(y = N_categories / 6, group = 1, color = "N Categorie"), 
            size = 1.5) +
  geom_point(aes(y = N_categories / 6, color = "N Categorie"), 
             size = 4) +
  annotate("text", x = 1.5, y = 3.5, 
           label = "Entropia alta = Community frammentata\n(divisa in molte categorie)",
           size = 3.5, color = "coral", fontface = "italic") +
  annotate("text", x = 1.5, y = 0.5, 
           label = "Entropia bassa = Community coesa\n(quasi tutta in una categoria)",
           size = 3.5, color = "darkgreen", fontface = "italic") +
  scale_y_continuous(
    name = "Entropia (frammentazione, 0-4 bits)",
    sec.axis = sec_axis(~.*6, name = "Numero Categorie")
  ) +
  scale_fill_manual(values = c("Entropia\n(frammentazione)" = "coral"), name = NULL) +
  scale_color_manual(values = c("N Categorie" = "steelblue"), name = NULL) +
  labs(title = "Frammentazione delle Community",
       subtitle = "Entropia di Shannon (barre) + Numero categorie presenti (linea)",
       x = NULL) +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold", hjust = 0.5, size = 15),
        plot.subtitle = element_text(hjust = 0.5, size = 11),
        axis.text.x = element_text(size = 10),
        legend.position = "bottom",
        legend.text = element_text(size = 11))

print(p_comm1)


# Plotto la purezza (lollipop chart) delle community

community_plot_data <- community_plot_data[order(-community_plot_data$Largest_pct), ]
community_plot_data$Community_short <- factor(community_plot_data$Community_short, 
                                              levels = community_plot_data$Community_short)

community_plot_data$Purezza_level <- cut(community_plot_data$Largest_pct,
                                         breaks = c(0, 50, 70, 100),
                                         labels = c("Bassa (<50%)", 
                                                    "Media (50-70%)", 
                                                    "Alta (>70%)"))

p_comm2 <- ggplot(community_plot_data, aes(x = Community_short, y = Largest_pct, 
                                           color = Purezza_level)) +
  geom_segment(aes(xend = Community_short, y = 0, yend = Largest_pct), 
               size = 1.5) +
  geom_point(size = 6) +
  geom_text(aes(label = paste0(round(Largest_pct, 1), "%")), 
            vjust = -1, size = 4, color = "black", fontface = "bold") +
  geom_hline(yintercept = 50, linetype = "dashed", color = "red", size = 0.8) +
  annotate("text", x = 0.7, y = 53, label = "Soglia 50%", 
           color = "red", size = 3.5) +
  scale_color_manual(values = c("Bassa (<50%)" = "#d62728", 
                                "Media (50-70%)" = "#ff7f0e",
                                "Alta (>70%)" = "#2ca02c"),
                     name = "Livello Purezza") +
  coord_flip() +
  labs(title = "Purezza delle Community Louvain",
       subtitle = "% nodi nella categoria dominante",
       x = NULL, y = "Purezza (%)") +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold", hjust = 0.5, size = 15),
        plot.subtitle = element_text(hjust = 0.5, size = 11),
        legend.position = "bottom",
        axis.text = element_text(size = 10))

print(p_comm2)


```




## 7 : Salvatoaggio Risultati ottenuti

```{r}
# Directory dei risultati
results_dir <- "../data/processed"

# Salva oggetti fondamentali (.rds)
saveRDS(results, file.path(results_dir, "community_detection_summary.rds"))
saveRDS(louvain_filtered, file.path(results_dir, "louvain_community_structure.rds"))

# Salva tabelle anche in formato csv
write.csv(results, file.path(results_dir, "community_detection_summary.csv"), row.names = FALSE)

cat("Risultati principali salvati nella cartella", results_dir, "\n")


```
